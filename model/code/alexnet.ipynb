{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o4EkV4TBbn3",
        "colab_type": "code",
        "outputId": "dbf2bc6a-ed51-4baa-fcf7-d1a7d03221b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMWToeD-B1Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -uq \"/content/drive/My Drive/Dataset/Data1.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JfJVVxEDkOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBHuSwVDzy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP_X1_fjD3t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "valid_datagen= ImageDataGenerator(rescale=1. / 255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7nyP3XcD7da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir=\"/content/Data/Train\"\n",
        "validation_data_dir=\"/content/Data/Valid\"\n",
        "img_height, img_width=227,227\n",
        "batch_size=16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLUXeUbyD-oZ",
        "colab_type": "code",
        "outputId": "ab175ee9-bf3d-4080-bd95-b7f7642c301a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6049 images belonging to 9 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W-YNY5jECXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_train_samples=6049"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVtxY5vmEFIT",
        "colab_type": "code",
        "outputId": "3b1c7b8d-67b6-47ae-ee04-9e7df00b055f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "validation_generator = valid_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1397 images belonging to 9 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWXRe3WSEIfL",
        "colab_type": "code",
        "outputId": "6696e8d0-11c0-465b-e9cb-f4032198201d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=r\"/content/Data/Test\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1863 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD2kPCvvEPAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_validation_samples=1397\n",
        "nb_test_samples=1863"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6SCIXcfESXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "import numpy as np\n",
        "np.random.seed(9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsZ7WnPwEqFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Instantiate an empty model\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANi-pMN3F7AG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKsiKbAEGXZG",
        "colab_type": "code",
        "outputId": "d846c2d3-3308-4ebd-b9b9-da9b729d6f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Passing it to a Fully Connected layer\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(4096, input_shape=(227*227*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 2nd Fully Connected Layer\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 3rd Fully Connected Layer\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1000))\n",
        "# model.add(Dense(9))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(9))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='best_model_pretrain.hdf5', verbose=1, save_best_only=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 55, 55, 96)        34944     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 55, 55, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 23, 23, 256)       614656    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 23, 23, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 9, 9, 384)         885120    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 9, 9, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 5, 5, 256)         884992    \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4096)              4198400   \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 9)                 9009      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 28,865,689\n",
            "Trainable params: 28,849,305\n",
            "Non-trainable params: 16,384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRKGSdJe1yGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unfreeze_layer_onwards(model, layer_name):\n",
        "    '''\n",
        "        This layer unfreezes all layers beyond layer_name\n",
        "    '''\n",
        "    trainable = False\n",
        "    for layer in model.layers:\n",
        "        try:\n",
        "            if layer.name == layer_name:\n",
        "                trainable = True\n",
        "            layer.trainable = trainable\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LxV5u-t2IW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_history(history, h):\n",
        "     '''\n",
        "\tThis function appends the statistics over epochs\n",
        "     '''\n",
        "     try:\n",
        "       history.history['loss'] = history.history['loss'] + h.history['loss']\n",
        "       history.history['val_loss'] = history.history['val_loss'] + h.history['val_loss']\n",
        "       history.history['acc'] = history.history['acc'] + h.history['acc']\n",
        "       history.history['val_acc'] = history.history['val_acc'] + h.history['val_acc']\n",
        "     except:\n",
        "       history = h\n",
        "                \n",
        "     return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMyfuW8Vx_Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = ['dense_9','dense_8','dense_7','conv2d_15','conv2d_14','conv2d_13','conv2d_12','conv2d_11']\n",
        "epochs = [10,10,10,10,10,10,10,10]\n",
        "lr = [1e-2,1e-3,1e-3,1e-3,1e-3,1e-3,1e-3,1e-3]\n",
        "from keras.optimizers import SGD\n",
        "history_finetune = []\n",
        "\n",
        "for i,layer in enumerate(layers):\n",
        "        \n",
        "    alexnet = unfreeze_layer_onwards(model,layer)    \n",
        "   \n",
        "    sgd = SGD(lr=lr[i], decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    alexnet.compile(loss='mse',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    for epoch in range(epochs[i]):    \n",
        "        h = alexnet.fit_generator(train_generator,\n",
        "                    steps_per_epoch = nb_train_samples //batch_size,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps= nb_validation_samples //batch_size,\n",
        "                    epochs=90,\n",
        "                    verbose=1,\n",
        "                    callbacks=[checkpointer])\n",
        "        \n",
        "        history_finetune = append_history(history_finetune,h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M1FeEhhH5b_",
        "colab_type": "code",
        "outputId": "0a99e62b-7c2e-4d41-8072-8b86aabc113c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# # Train Model\n",
        "# history = model.fit(\n",
        "#         train_generator,\n",
        "#         epochs=150,\n",
        "#         validation_data = validation_generator, \n",
        "#         callbacks=[checkpointer]\n",
        "#         )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:725: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 2. Skipping tag 41487\n",
            "  \" Skipping tag %s\" % (size, len(data), tag))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:725: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 41988\n",
            "  \" Skipping tag %s\" % (size, len(data), tag))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "379/379 [==============================] - 146s 385ms/step - loss: 2.9285 - acc: 0.1435 - val_loss: 2.2272 - val_acc: 0.1475\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.22717, saving model to best_model_pretrain.hdf5\n",
            "Epoch 2/150\n",
            "379/379 [==============================] - 137s 361ms/step - loss: 2.6790 - acc: 0.1557 - val_loss: 2.4596 - val_acc: 0.1976\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 2.22717\n",
            "Epoch 3/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 2.4996 - acc: 0.1616 - val_loss: 2.0564 - val_acc: 0.2119\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.22717 to 2.05640, saving model to best_model_pretrain.hdf5\n",
            "Epoch 4/150\n",
            "379/379 [==============================] - 137s 361ms/step - loss: 2.4031 - acc: 0.1618 - val_loss: 2.2721 - val_acc: 0.1224\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 2.05640\n",
            "Epoch 5/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 2.3120 - acc: 0.1694 - val_loss: 2.2715 - val_acc: 0.1496\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 2.05640\n",
            "Epoch 6/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 2.2625 - acc: 0.1740 - val_loss: 2.0585 - val_acc: 0.1918\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.05640\n",
            "Epoch 7/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 2.2118 - acc: 0.1799 - val_loss: 2.0388 - val_acc: 0.2155\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.05640 to 2.03878, saving model to best_model_pretrain.hdf5\n",
            "Epoch 8/150\n",
            "379/379 [==============================] - 140s 368ms/step - loss: 2.1824 - acc: 0.1783 - val_loss: 2.1071 - val_acc: 0.1625\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.03878\n",
            "Epoch 9/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 2.1451 - acc: 0.1928 - val_loss: 1.9967 - val_acc: 0.2155\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.03878 to 1.99667, saving model to best_model_pretrain.hdf5\n",
            "Epoch 10/150\n",
            "379/379 [==============================] - 139s 368ms/step - loss: 2.1300 - acc: 0.1872 - val_loss: 2.0807 - val_acc: 0.2105\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.99667\n",
            "Epoch 11/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 2.1043 - acc: 0.1923 - val_loss: 2.0067 - val_acc: 0.2262\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.99667\n",
            "Epoch 12/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 2.0976 - acc: 0.1926 - val_loss: 2.0255 - val_acc: 0.2162\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.99667\n",
            "Epoch 13/150\n",
            "379/379 [==============================] - 141s 371ms/step - loss: 2.0725 - acc: 0.1990 - val_loss: 2.0087 - val_acc: 0.2105\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.99667\n",
            "Epoch 14/150\n",
            "379/379 [==============================] - 140s 369ms/step - loss: 2.0743 - acc: 0.1943 - val_loss: 1.9624 - val_acc: 0.2369\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.99667 to 1.96238, saving model to best_model_pretrain.hdf5\n",
            "Epoch 15/150\n",
            "379/379 [==============================] - 139s 368ms/step - loss: 2.0632 - acc: 0.2079 - val_loss: 1.9828 - val_acc: 0.2083\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.96238\n",
            "Epoch 16/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 2.0548 - acc: 0.2053 - val_loss: 2.2141 - val_acc: 0.1467\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.96238\n",
            "Epoch 17/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 2.0572 - acc: 0.2066 - val_loss: 2.0521 - val_acc: 0.2212\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.96238\n",
            "Epoch 18/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 2.0318 - acc: 0.2093 - val_loss: 1.9887 - val_acc: 0.2283\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.96238\n",
            "Epoch 19/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 2.0122 - acc: 0.2277 - val_loss: 1.9188 - val_acc: 0.2462\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.96238 to 1.91875, saving model to best_model_pretrain.hdf5\n",
            "Epoch 20/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 2.0101 - acc: 0.2259 - val_loss: 1.9248 - val_acc: 0.2892\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.91875\n",
            "Epoch 21/150\n",
            "379/379 [==============================] - 136s 360ms/step - loss: 1.9985 - acc: 0.2307 - val_loss: 1.9845 - val_acc: 0.2591\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.91875\n",
            "Epoch 22/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.9742 - acc: 0.2442 - val_loss: 1.9247 - val_acc: 0.2777\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.91875\n",
            "Epoch 23/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.9880 - acc: 0.2352 - val_loss: 1.9503 - val_acc: 0.2713\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.91875\n",
            "Epoch 24/150\n",
            "379/379 [==============================] - 137s 363ms/step - loss: 1.9531 - acc: 0.2516 - val_loss: 1.9951 - val_acc: 0.2341\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.91875\n",
            "Epoch 25/150\n",
            "379/379 [==============================] - 137s 361ms/step - loss: 1.9387 - acc: 0.2507 - val_loss: 1.8836 - val_acc: 0.2921\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.91875 to 1.88358, saving model to best_model_pretrain.hdf5\n",
            "Epoch 26/150\n",
            "379/379 [==============================] - 136s 360ms/step - loss: 1.9350 - acc: 0.2615 - val_loss: 1.9166 - val_acc: 0.2734\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.88358\n",
            "Epoch 27/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 1.9210 - acc: 0.2665 - val_loss: 2.1256 - val_acc: 0.2276\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.88358\n",
            "Epoch 28/150\n",
            "379/379 [==============================] - 136s 359ms/step - loss: 1.9102 - acc: 0.2752 - val_loss: 1.8406 - val_acc: 0.2913\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.88358 to 1.84064, saving model to best_model_pretrain.hdf5\n",
            "Epoch 29/150\n",
            "379/379 [==============================] - 135s 357ms/step - loss: 1.8734 - acc: 0.2889 - val_loss: 2.2414 - val_acc: 0.2462\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.84064\n",
            "Epoch 30/150\n",
            "379/379 [==============================] - 136s 359ms/step - loss: 1.8717 - acc: 0.2916 - val_loss: 1.9852 - val_acc: 0.2949\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.84064\n",
            "Epoch 31/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.8604 - acc: 0.2944 - val_loss: 2.2010 - val_acc: 0.2448\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.84064\n",
            "Epoch 32/150\n",
            "379/379 [==============================] - 137s 361ms/step - loss: 1.8471 - acc: 0.2950 - val_loss: 2.0045 - val_acc: 0.2963\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.84064\n",
            "Epoch 33/150\n",
            "379/379 [==============================] - 137s 361ms/step - loss: 1.8295 - acc: 0.3120 - val_loss: 1.8046 - val_acc: 0.3114\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.84064 to 1.80465, saving model to best_model_pretrain.hdf5\n",
            "Epoch 34/150\n",
            "379/379 [==============================] - 137s 361ms/step - loss: 1.8274 - acc: 0.3036 - val_loss: 2.0742 - val_acc: 0.2341\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.80465\n",
            "Epoch 35/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 1.8165 - acc: 0.3140 - val_loss: 2.3954 - val_acc: 0.1868\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.80465\n",
            "Epoch 36/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.7966 - acc: 0.3188 - val_loss: 1.9835 - val_acc: 0.2863\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.80465\n",
            "Epoch 37/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.7738 - acc: 0.3381 - val_loss: 1.7644 - val_acc: 0.3414\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.80465 to 1.76436, saving model to best_model_pretrain.hdf5\n",
            "Epoch 38/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.7668 - acc: 0.3349 - val_loss: 1.7944 - val_acc: 0.3300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.76436\n",
            "Epoch 39/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.7497 - acc: 0.3471 - val_loss: 1.7640 - val_acc: 0.3393\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.76436 to 1.76403, saving model to best_model_pretrain.hdf5\n",
            "Epoch 40/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.7387 - acc: 0.3465 - val_loss: 1.8090 - val_acc: 0.3293\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.76403\n",
            "Epoch 41/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.7020 - acc: 0.3631 - val_loss: 1.7303 - val_acc: 0.3608\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.76403 to 1.73028, saving model to best_model_pretrain.hdf5\n",
            "Epoch 42/150\n",
            "379/379 [==============================] - 139s 365ms/step - loss: 1.6849 - acc: 0.3699 - val_loss: 2.1514 - val_acc: 0.2827\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.73028\n",
            "Epoch 43/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.6876 - acc: 0.3628 - val_loss: 1.6724 - val_acc: 0.4137\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.73028 to 1.67244, saving model to best_model_pretrain.hdf5\n",
            "Epoch 44/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.6697 - acc: 0.3823 - val_loss: 1.8326 - val_acc: 0.3386\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.67244\n",
            "Epoch 45/150\n",
            "379/379 [==============================] - 137s 363ms/step - loss: 1.6502 - acc: 0.3874 - val_loss: 2.0034 - val_acc: 0.3028\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.67244\n",
            "Epoch 46/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.6441 - acc: 0.3917 - val_loss: 1.6192 - val_acc: 0.3873\n",
            "\n",
            "Epoch 00046: val_loss improved from 1.67244 to 1.61923, saving model to best_model_pretrain.hdf5\n",
            "Epoch 47/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.6143 - acc: 0.4062 - val_loss: 1.8823 - val_acc: 0.3565\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.61923\n",
            "Epoch 48/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.5854 - acc: 0.4119 - val_loss: 1.7306 - val_acc: 0.3701\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.61923\n",
            "Epoch 49/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.5812 - acc: 0.4197 - val_loss: 1.7217 - val_acc: 0.4037\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.61923\n",
            "Epoch 50/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 1.5788 - acc: 0.4199 - val_loss: 1.6983 - val_acc: 0.4087\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.61923\n",
            "Epoch 51/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.5453 - acc: 0.4413 - val_loss: 1.6591 - val_acc: 0.4037\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.61923\n",
            "Epoch 52/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.5413 - acc: 0.4357 - val_loss: 1.9630 - val_acc: 0.3393\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.61923\n",
            "Epoch 53/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.5274 - acc: 0.4385 - val_loss: 1.5485 - val_acc: 0.4223\n",
            "\n",
            "Epoch 00053: val_loss improved from 1.61923 to 1.54846, saving model to best_model_pretrain.hdf5\n",
            "Epoch 54/150\n",
            "379/379 [==============================] - 139s 368ms/step - loss: 1.4968 - acc: 0.4522 - val_loss: 1.5185 - val_acc: 0.4424\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.54846 to 1.51848, saving model to best_model_pretrain.hdf5\n",
            "Epoch 55/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.4882 - acc: 0.4533 - val_loss: 1.8765 - val_acc: 0.3694\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.51848\n",
            "Epoch 56/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.4838 - acc: 0.4509 - val_loss: 1.6868 - val_acc: 0.4109\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.51848\n",
            "Epoch 57/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.4682 - acc: 0.4685 - val_loss: 1.7074 - val_acc: 0.4188\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.51848\n",
            "Epoch 58/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.4553 - acc: 0.4741 - val_loss: 1.8014 - val_acc: 0.4145\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.51848\n",
            "Epoch 59/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.4252 - acc: 0.4810 - val_loss: 2.3581 - val_acc: 0.2785\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.51848\n",
            "Epoch 60/150\n",
            "379/379 [==============================] - 140s 370ms/step - loss: 1.4296 - acc: 0.4758 - val_loss: 1.7681 - val_acc: 0.4195\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.51848\n",
            "Epoch 61/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.4018 - acc: 0.4985 - val_loss: 1.5080 - val_acc: 0.4767\n",
            "\n",
            "Epoch 00061: val_loss improved from 1.51848 to 1.50801, saving model to best_model_pretrain.hdf5\n",
            "Epoch 62/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.3836 - acc: 0.4934 - val_loss: 1.8574 - val_acc: 0.4281\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.50801\n",
            "Epoch 63/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 1.3726 - acc: 0.5076 - val_loss: 1.6831 - val_acc: 0.4545\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.50801\n",
            "Epoch 64/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.3504 - acc: 0.5068 - val_loss: 1.6565 - val_acc: 0.4717\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.50801\n",
            "Epoch 65/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.3368 - acc: 0.5155 - val_loss: 1.5866 - val_acc: 0.4653\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.50801\n",
            "Epoch 66/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.3210 - acc: 0.5234 - val_loss: 1.6059 - val_acc: 0.4689\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.50801\n",
            "Epoch 67/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 1.2925 - acc: 0.5305 - val_loss: 1.4498 - val_acc: 0.4932\n",
            "\n",
            "Epoch 00067: val_loss improved from 1.50801 to 1.44984, saving model to best_model_pretrain.hdf5\n",
            "Epoch 68/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 1.2672 - acc: 0.5427 - val_loss: 1.7023 - val_acc: 0.4467\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.44984\n",
            "Epoch 69/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 1.2633 - acc: 0.5452 - val_loss: 1.4060 - val_acc: 0.4961\n",
            "\n",
            "Epoch 00069: val_loss improved from 1.44984 to 1.40603, saving model to best_model_pretrain.hdf5\n",
            "Epoch 70/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.2584 - acc: 0.5571 - val_loss: 1.5846 - val_acc: 0.4953\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.40603\n",
            "Epoch 71/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.2344 - acc: 0.5559 - val_loss: 1.7223 - val_acc: 0.4474\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.40603\n",
            "Epoch 72/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 1.2116 - acc: 0.5701 - val_loss: 1.5306 - val_acc: 0.4853\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.40603\n",
            "Epoch 73/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.1919 - acc: 0.5749 - val_loss: 1.3583 - val_acc: 0.5147\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.40603 to 1.35831, saving model to best_model_pretrain.hdf5\n",
            "Epoch 74/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.1666 - acc: 0.5843 - val_loss: 1.8739 - val_acc: 0.4424\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.35831\n",
            "Epoch 75/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.1591 - acc: 0.5862 - val_loss: 1.6694 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.35831\n",
            "Epoch 76/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 1.1427 - acc: 0.5886 - val_loss: 1.4202 - val_acc: 0.5233\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.35831\n",
            "Epoch 77/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.1294 - acc: 0.5983 - val_loss: 2.0037 - val_acc: 0.4288\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.35831\n",
            "Epoch 78/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.1149 - acc: 0.6011 - val_loss: 1.8500 - val_acc: 0.4646\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.35831\n",
            "Epoch 79/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 1.0779 - acc: 0.6151 - val_loss: 1.7557 - val_acc: 0.4553\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.35831\n",
            "Epoch 80/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 1.0724 - acc: 0.6181 - val_loss: 1.6050 - val_acc: 0.5161\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.35831\n",
            "Epoch 81/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 1.0617 - acc: 0.6220 - val_loss: 1.6371 - val_acc: 0.4846\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.35831\n",
            "Epoch 82/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 1.0037 - acc: 0.6417 - val_loss: 1.6281 - val_acc: 0.4853\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.35831\n",
            "Epoch 83/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 1.0265 - acc: 0.6410 - val_loss: 1.7691 - val_acc: 0.4710\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.35831\n",
            "Epoch 84/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.9910 - acc: 0.6405 - val_loss: 1.5290 - val_acc: 0.5283\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.35831\n",
            "Epoch 85/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 0.9622 - acc: 0.6595 - val_loss: 1.4763 - val_acc: 0.5268\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.35831\n",
            "Epoch 86/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 0.9487 - acc: 0.6659 - val_loss: 1.5308 - val_acc: 0.5268\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.35831\n",
            "Epoch 87/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.9247 - acc: 0.6679 - val_loss: 1.4484 - val_acc: 0.5376\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.35831\n",
            "Epoch 88/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 0.8975 - acc: 0.6834 - val_loss: 1.5152 - val_acc: 0.5261\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.35831\n",
            "Epoch 89/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.8820 - acc: 0.6911 - val_loss: 1.6617 - val_acc: 0.5011\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.35831\n",
            "Epoch 90/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.8675 - acc: 0.6967 - val_loss: 1.9192 - val_acc: 0.4703\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.35831\n",
            "Epoch 91/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 0.8605 - acc: 0.7004 - val_loss: 1.8077 - val_acc: 0.4717\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.35831\n",
            "Epoch 92/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 0.8348 - acc: 0.7050 - val_loss: 1.4474 - val_acc: 0.5648\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.35831\n",
            "Epoch 93/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.8012 - acc: 0.7167 - val_loss: 1.6355 - val_acc: 0.5011\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.35831\n",
            "Epoch 94/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.7905 - acc: 0.7258 - val_loss: 1.6869 - val_acc: 0.5247\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.35831\n",
            "Epoch 95/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.7647 - acc: 0.7302 - val_loss: 1.7620 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.35831\n",
            "Epoch 96/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.7379 - acc: 0.7396 - val_loss: 1.7390 - val_acc: 0.5075\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.35831\n",
            "Epoch 97/150\n",
            "379/379 [==============================] - 139s 365ms/step - loss: 0.7395 - acc: 0.7383 - val_loss: 1.5493 - val_acc: 0.5519\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.35831\n",
            "Epoch 98/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.7078 - acc: 0.7467 - val_loss: 1.6602 - val_acc: 0.5497\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.35831\n",
            "Epoch 99/150\n",
            "379/379 [==============================] - 139s 365ms/step - loss: 0.6795 - acc: 0.7640 - val_loss: 1.7648 - val_acc: 0.5211\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.35831\n",
            "Epoch 100/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 0.6457 - acc: 0.7742 - val_loss: 1.8445 - val_acc: 0.5104\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.35831\n",
            "Epoch 101/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.6538 - acc: 0.7677 - val_loss: 1.6510 - val_acc: 0.5455\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.35831\n",
            "Epoch 102/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 0.6267 - acc: 0.7853 - val_loss: 1.9816 - val_acc: 0.5032\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.35831\n",
            "Epoch 103/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.5893 - acc: 0.7919 - val_loss: 1.6369 - val_acc: 0.5497\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.35831\n",
            "Epoch 104/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.5886 - acc: 0.7939 - val_loss: 1.8190 - val_acc: 0.5412\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.35831\n",
            "Epoch 105/150\n",
            "379/379 [==============================] - 138s 363ms/step - loss: 0.5542 - acc: 0.8019 - val_loss: 1.9703 - val_acc: 0.4810\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.35831\n",
            "Epoch 106/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.5508 - acc: 0.8117 - val_loss: 2.1761 - val_acc: 0.5075\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.35831\n",
            "Epoch 107/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.5187 - acc: 0.8184 - val_loss: 2.1018 - val_acc: 0.5168\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.35831\n",
            "Epoch 108/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.5106 - acc: 0.8204 - val_loss: 1.9093 - val_acc: 0.5354\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.35831\n",
            "Epoch 109/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.4925 - acc: 0.8242 - val_loss: 1.9552 - val_acc: 0.5591\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.35831\n",
            "Epoch 110/150\n",
            "379/379 [==============================] - 139s 366ms/step - loss: 0.4758 - acc: 0.8363 - val_loss: 1.8019 - val_acc: 0.5634\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.35831\n",
            "Epoch 111/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.4599 - acc: 0.8402 - val_loss: 1.8823 - val_acc: 0.5655\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.35831\n",
            "Epoch 112/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.4462 - acc: 0.8442 - val_loss: 1.8600 - val_acc: 0.5626\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.35831\n",
            "Epoch 113/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.4508 - acc: 0.8424 - val_loss: 1.7974 - val_acc: 0.5576\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.35831\n",
            "Epoch 114/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.4213 - acc: 0.8539 - val_loss: 2.2028 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.35831\n",
            "Epoch 115/150\n",
            "379/379 [==============================] - 137s 362ms/step - loss: 0.3852 - acc: 0.8640 - val_loss: 1.9887 - val_acc: 0.5741\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.35831\n",
            "Epoch 116/150\n",
            "379/379 [==============================] - 137s 363ms/step - loss: 0.3998 - acc: 0.8635 - val_loss: 2.1244 - val_acc: 0.5433\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.35831\n",
            "Epoch 117/150\n",
            "379/379 [==============================] - 137s 363ms/step - loss: 0.3816 - acc: 0.8694 - val_loss: 2.2142 - val_acc: 0.5354\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.35831\n",
            "Epoch 118/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.3721 - acc: 0.8676 - val_loss: 2.0889 - val_acc: 0.5354\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.35831\n",
            "Epoch 119/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.3486 - acc: 0.8765 - val_loss: 2.4690 - val_acc: 0.5340\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.35831\n",
            "Epoch 120/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.3330 - acc: 0.8890 - val_loss: 2.0403 - val_acc: 0.5412\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.35831\n",
            "Epoch 121/150\n",
            "379/379 [==============================] - 138s 364ms/step - loss: 0.3200 - acc: 0.8908 - val_loss: 2.1128 - val_acc: 0.5591\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.35831\n",
            "Epoch 122/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 0.3224 - acc: 0.8903 - val_loss: 2.1380 - val_acc: 0.5576\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.35831\n",
            "Epoch 123/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 0.3077 - acc: 0.8905 - val_loss: 2.2735 - val_acc: 0.5290\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.35831\n",
            "Epoch 124/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.2918 - acc: 0.9024 - val_loss: 2.3328 - val_acc: 0.5268\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.35831\n",
            "Epoch 125/150\n",
            "379/379 [==============================] - 137s 363ms/step - loss: 0.2989 - acc: 0.8978 - val_loss: 2.2251 - val_acc: 0.5662\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.35831\n",
            "Epoch 126/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.2824 - acc: 0.9042 - val_loss: 2.4974 - val_acc: 0.5254\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.35831\n",
            "Epoch 127/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.2673 - acc: 0.9070 - val_loss: 2.4264 - val_acc: 0.5347\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.35831\n",
            "Epoch 128/150\n",
            "379/379 [==============================] - 139s 367ms/step - loss: 0.2918 - acc: 0.8996 - val_loss: 2.0728 - val_acc: 0.5662\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.35831\n",
            "Epoch 129/150\n",
            "379/379 [==============================] - 138s 365ms/step - loss: 0.2664 - acc: 0.9091 - val_loss: 2.3344 - val_acc: 0.5512\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.35831\n",
            "Epoch 130/150\n",
            "360/379 [===========================>..] - ETA: 6s - loss: 0.2583 - acc: 0.9106"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23v5zo2yIGWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fh4AlISkr5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yeFaSQhkwjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnPbz0N0ksij",
        "colab_type": "text"
      },
      "source": [
        "load alexnet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy9cNkXrkxtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz2Rw7DLkzbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}